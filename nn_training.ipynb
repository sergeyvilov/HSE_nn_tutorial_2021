{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18317106-68d2-495f-af33-764fa228cdb0",
   "metadata": {},
   "source": [
    "## Neural Network training\n",
    "\n",
    "\n",
    "We have prepared for you an extract of 3000 variant images for Genome in a Bottle NA12878 sample.\n",
    "Half images are high-quality GiB variants, half images are sequencing artifacts.\n",
    "The chosen variants are uniformly distributed on the whole genome length.\n",
    "\n",
    "Having all this data, we are ready to train the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1816235a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf64950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get tutorial data\n",
    "\n",
    "!wget -O data.tar 'https://drive.google.com/uc?export=download&id=1_FVYYq_GweoBNxNVysJEYw8x_rPtmQnd'\n",
    "!tar -xvf data.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67750d1e-5fc8-45f1-a6eb-dc91bddbd4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ARCHIVE_PATH = 'data/dataset/tutorial_GiB.zip'\n",
    "\n",
    "IMAGES_LIST = 'data/dataset/all_images_list' #relative paths to all images in the dataset archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30d01e2-29db-46c5-8132-fee14e3c7654",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 5 {IMAGES_LIST}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622c68d8-43ba-40bd-813a-638a2baae358",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv(IMAGES_LIST, names=['relative_path']) # list of all images as pandas dataframe\n",
    "dataset_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb2b777-bdb9-4ce1-8125-388e8b8c7899",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> Now we need to add an extra column which should be 1 for germline variants and 0 for sequencing artifacts. How would one implement this? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b4ffe-19a3-4b1e-8084-30b81a5d6016",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df['labels'] = ... #label=1 if relative_path begins with 'germline'\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d25770-b9d9-42e4-9457-297757a70619",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> Now let's split our dataset_df into train_df and val_df s.t. 90% of data goes to the train_df and the remaining 90% goes to val_df. How can this be implemented?  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ba6e44-6ef2-4dae-b9ae-c6c20327b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = ...\n",
    "\n",
    "val_df = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4077dd-efb9-40ae-af3f-30eb0e4386a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the class distribution in train and validation datasets\n",
    "\n",
    "print('Train dataframe class counts:')\n",
    "print(train_df['labels'].value_counts())\n",
    "print()\n",
    "print('Validation dataframe class counts:')\n",
    "val_df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5530c28c-d9e7-4003-986f-3237ef019ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667c9d88-a92b-48ed-be46-89da1341a63b",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> What's wrong with our train dataframe? How to change this? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd1d191-7a06-4290-9f7f-fa063a4c10d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = ...\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd92fc7-fcc7-44d6-a62f-b908dff05751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "import pickle\n",
    "import zipfile\n",
    "\n",
    "dataset_archive = zipfile.ZipFile(DATASET_ARCHIVE_PATH, 'r') #open the archive with all variant images in the dataset\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    \n",
    "    '''\n",
    "    Dataset of variant images\n",
    "    '''\n",
    "\n",
    "    def __init__(self, \n",
    "                 data,           #relative path to images with corresponding labels\n",
    "                 target_height,  #target image height for the neural network\n",
    "                ):\n",
    "\n",
    "        self.data = data\n",
    "        self.target_height = target_height\n",
    "        \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        '''\n",
    "        Retrive an image\n",
    "                \n",
    "        If image height is smaller than self.target_height,\n",
    "        we pad it with \"N\" to reach the required self.target_height.\n",
    "        If image height is larger that self.target_height,\n",
    "        we remove some reads on the top and on the bottom, leaving the central part.\n",
    "        \n",
    "        The original image is then shifted to the center of this new full image.\n",
    "        '''\n",
    "        \n",
    "        relative_image_path, label = self.data[idx]\n",
    "        \n",
    "        #full_image_path = self.dataset_dir+relative_image_path\n",
    "        #with open(full_image_path, 'rb') as imgfile:\n",
    "        #        image = pickle.load(imgfile)\n",
    "\n",
    "        imgfile = dataset_archive.open(relative_image_path) #open image file directly out of the archive\n",
    "        image = pickle.load(imgfile)                #load pickle data from the image file\n",
    "            \n",
    "        one_hot_ref = image['one_hot_ref']      #one-hot encoding of reference bases\n",
    "        p_hot_reads = image['p_hot_reads']*1e-4 #p-hot encoding of reads bases\n",
    "        flags_reads = image['flags_reads']      #flags for reads\n",
    "            \n",
    "        image_height, image_width, _ = p_hot_reads.shape\n",
    "                  \n",
    "        one_hot_ref = np.tile(one_hot_ref, (image_height,1,1)) #propagate reference bases over all reads\n",
    "            \n",
    "        image = np.concatenate((one_hot_ref,p_hot_reads,flags_reads), axis=2)\n",
    "\n",
    "        if self.target_height>image_height:\n",
    "            #pad image with 'N' to reach the target height\n",
    "            padding_image = np.concatenate(\n",
    "                    (np.tile(one_hot_ref[0,:,:],(self.target_height-image_height,1,1)), #4 channels for reference bases, should be the same as in the reads\n",
    "                    np.ones((self.target_height-image_height, image_width, 4))*0.25,    #p-hot encoding for 'N'\n",
    "                    np.zeros((self.target_height-image_height, image_width, 6))         #read flags\n",
    "                    ), \n",
    "                    axis=2)\n",
    "            \n",
    "            full_image = np.concatenate((image, padding_image), axis = 0) #concatenate over the reads axis\n",
    "            full_image = np.roll(full_image,max(self.target_height//2-image_height//2,0),axis=0) #put the piledup reads in the center of image\n",
    "\n",
    "        else:\n",
    "            #if there are too many reads, keep reads in the center, remove at the top and at the bottom\n",
    "            shift = max(image_height//2-self.target_height//2,0)\n",
    "            full_image = image[shift:shift+self.target_height,:,:]\n",
    "                \n",
    "        full_image = np.transpose(full_image, (2,0,1)) #change dimensions order to CxWxH\n",
    "\n",
    "        return full_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55380343-ef58-44b6-9106-7a838966b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define train and validation datasets\n",
    "\n",
    "train_dataset = Dataset(train_df.values.tolist(), target_height=70)\n",
    "\n",
    "val_dataset = Dataset(val_df.values.tolist(), target_height=70)\n",
    "\n",
    "#define train and validation dataloaders\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=96, shuffle=False, num_workers=0)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=96, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf05595-4e04-4802-87a2-d5297028ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the neural network architecture\n",
    "\n",
    "class ConvNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, dropout=0.):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        #150x70\n",
    "\n",
    "        self.conv1 = nn.Conv2d(14, 32, kernel_size=5, stride=1, padding=0) #146x66\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.dp1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=0) #142x62\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.dp2 = nn.Dropout(dropout)\n",
    "        self.mp2 = nn.MaxPool2d(kernel_size=2, stride=2)#71x31x64\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=0)#67x27\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.dp3 = nn.Dropout(dropout)\n",
    "        self.mp3 = nn.MaxPool2d(kernel_size=2, stride=2)#33x13\n",
    "\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=0)#29x9\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.act4 = nn.ReLU()\n",
    "        self.dp4 = nn.Dropout(dropout)\n",
    "        self.mp4 = nn.MaxPool2d(kernel_size=2, stride=2)#14x4\n",
    "\n",
    "        self.flt = nn.Flatten()\n",
    "\n",
    "        self.fc5 = nn.Linear(14 * 4 * 64, 64)\n",
    "        self.bn5 = nn.BatchNorm1d(64)\n",
    "        self.act5 = nn.ReLU()\n",
    "        self.dp5 = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc6 = nn.Linear(64, 1)\n",
    "        self.act6 = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.act1(out)\n",
    "        out = self.dp1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.act2(out)\n",
    "        out = self.dp2(out)\n",
    "        out = self.mp2(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.act3(out)\n",
    "        out = self.dp3(out)\n",
    "        out = self.mp3(out)\n",
    "\n",
    "        out = self.conv4(out)\n",
    "        out = self.bn4(out)\n",
    "        out = self.act4(out)\n",
    "        out = self.dp4(out)\n",
    "        out = self.mp4(out)\n",
    "\n",
    "        out = self.flt(out)\n",
    "\n",
    "        out = self.fc5(out)\n",
    "        out = self.bn5(out)\n",
    "        out = self.act5(out)\n",
    "        out = self.dp5(out)\n",
    "\n",
    "        out = self.fc6(out)\n",
    "        out = self.act6(out)\n",
    "        \n",
    "        out = torch.squeeze(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96436c89-0b6f-4870-82f6-3e7904304806",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get access to GPU\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('\\nCUDA device: GPU\\n')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('\\nCUDA device: CPU\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeb462e-a132-4101-b6ea-321a7c8e7f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNN(dropout=0.2) #create the model\n",
    "\n",
    "model = model.to(device)    #model to CUDA\n",
    "\n",
    "model_params = [p for p in model.parameters() if p.requires_grad] #model parameters for optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f832d28-5184-4701-a1bf-44f8193c1d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the model architecture\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(model,(14,150,70), batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0029c6f3-2ec2-4847-be52-0f67a1acd30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model_params, lr=2e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26fdf0a-b621-4e35-90d9-fc7b728b03b7",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> We shall implement a function to train the model. What loss function should we choose ? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483cac52-8cb0-4010-bfff-b446c36f852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train(model, dataloader):\n",
    "    \n",
    "    model.train() #model to train mode\n",
    "   \n",
    "    criterion = ...\n",
    "    \n",
    "    tot_itr = len(dataloader.dataset.data)//dataloader.batch_size #total train iterations\n",
    "    \n",
    "    pbar = tqdm(total = tot_itr, ncols=700) #progress bar\n",
    "    \n",
    "    beta = 0.98 #beta of running average, don't change\n",
    "    \n",
    "    avg_loss = 0. #average loss\n",
    "    \n",
    "    for itr_idx, (images, labels) in enumerate(dataloader):\n",
    "        \n",
    "        images = images.to(torch.float).to(device) #images to torch.float32 then to GPU\n",
    "        labels = labels.to(torch.float).to(device) #labels to torch.float32 then to GPU\n",
    "\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #exponential moving evaraging of loss\n",
    "        avg_loss = beta * avg_loss + (1-beta)*loss.item()\n",
    "        smoothed_loss = avg_loss / (1 - beta**(itr_idx+1))\n",
    " \n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f\"Running loss:{smoothed_loss:.4}\")\n",
    "        \n",
    "    return smoothed_loss\n",
    "        \n",
    "train(model, train_dataloader);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8b48a7-9e6a-4027-b9e1-da332d3251a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader):\n",
    "    \n",
    "    model.eval() #model to validation mode\n",
    "   \n",
    "    criterion = ...\n",
    "    \n",
    "    tot_itr = len(dataloader.dataset.data)//dataloader.batch_size #total validation iterations\n",
    "    \n",
    "    pbar = tqdm(total = tot_itr, ncols=700) #progress bar\n",
    "    \n",
    "    all_loss = 0. #all losses, for simple averaging\n",
    "    \n",
    "    all_preds = [] #all validation predictions\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for itr_idx, (images, labels) in enumerate(dataloader):\n",
    "        \n",
    "            images = images.to(torch.float).to(device) #images to torch.float32 then to GPU\n",
    "            labels = labels.to(torch.float).to(device) #labels to torch.float32 then to GPU\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels) \n",
    "\n",
    "            all_loss += loss.item()\n",
    "            \n",
    "            all_preds.extend(list(zip(outputs.cpu().numpy(), labels.cpu().numpy())))\n",
    "             \n",
    "            pbar.update(1)\n",
    "            pbar.set_description(f\"Running loss:{all_loss/(itr_idx+1):.4}\")\n",
    "        \n",
    "    return all_loss/(itr_idx+1), all_preds #return average loss and predictions\n",
    "\n",
    "val_loss, val_preds = validate(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d88cc-58f4-4df0-a0f7-5db845854c2e",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> By the way, why do we use exponential moving averaging for training and simple aberaging during validation ? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029cff6a-8cd7-4da7-a1ca-f960843c97cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute precision-recall curve and get the maximum f1-score\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "    \n",
    "outputs, labels = list(zip(*val_preds))\n",
    "\n",
    "all_precision, all_recall, thresholds = precision_recall_curve(labels, outputs)\n",
    "\n",
    "f1 = 2*all_precision*all_recall/(all_precision+all_recall+1e-10)\n",
    "\n",
    "best_f1 = max(f1)\n",
    "\n",
    "best_f1_index = np.nanargmax(f1) #index of the maximal f1-score to get the corresponding precision and recall\n",
    "\n",
    "print(f'Maximal f1-score {best_f1:.2} is reached with precision {all_precision[best_f1_index]:.2} and recall {all_recall[best_f1_index]:.2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5896b762-2e70-48c0-996e-e5e21c7b7727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot precision-recall curve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_recall, all_precision, 'r-', lw=2, marker = '.', markersize = 5)\n",
    "plt.grid()\n",
    "plt.ylabel('Precision', fontsize=15)\n",
    "plt.xlabel('Recall', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ca43ae-af51-4f8c-858c-75d6e2d5fe5b",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> In our train set we had the class proportion true:false = 1:1. For an actual GiB NA12878 sample the class proportion is true:false = 1:2. Do we have to do anything with this class imbalance and if we do then what? </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
